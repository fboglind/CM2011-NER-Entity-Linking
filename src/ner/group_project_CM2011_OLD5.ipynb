{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a038a381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fredrik/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import tqdm as notebook_tqdm\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from seqeval.metrics import (precision_score,\n",
    "                             recall_score,\n",
    "                             f1_score,\n",
    "                             classification_report)\n",
    "\n",
    "from datasets import (load_dataset,\n",
    "                      DatasetDict, \n",
    "                      Features, \n",
    "                      Sequence, \n",
    "                      ClassLabel, \n",
    "                      Value, \n",
    "                      interleave_datasets, \n",
    "                      get_dataset_config_names, \n",
    "                      load_dataset, \n",
    "                      load_from_disk\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "from utility_functions import (split_sources,\n",
    "                               whitespace_tokens_with_spans,\n",
    "                               spans_to_bio_labels,\n",
    "                               build_bio_label_list_from_sources, \n",
    "                               compute_metrics_factory, \n",
    "                               make_to_features, \n",
    "                               process_all,\n",
    "                               normalize_types\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571229ac",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c75b2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available configurations:\n",
      "- 1177\n",
      "- lt\n",
      "- wiki\n"
     ]
    }
   ],
   "source": [
    "# We list all available configurations of the dataset:\n",
    "# configs = get_dataset_config_names(\"bigbio/swedish_medical_ner\")\n",
    "configs = get_dataset_config_names(\"community-datasets/swedish_medical_ner\")\n",
    "print(\"Available configurations:\")\n",
    "for config in configs:\n",
    "    print(f\"- {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ba4a7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration: 1177 from the huggingface hub\n",
      "- 1177\n",
      "{'entities': Sequence(feature={'end': Value(dtype='int32', id=None),\n",
      "                               'start': Value(dtype='int32', id=None),\n",
      "                               'text': Value(dtype='string', id=None),\n",
      "                               'type': ClassLabel(names=['Disorder and Finding',\n",
      "                                                         'Pharmaceutical Drug',\n",
      "                                                         'Body Structure'],\n",
      "                                                  id=None)},\n",
      "                      length=-1,\n",
      "                      id=None),\n",
      " 'sentence': Value(dtype='string', id=None),\n",
      " 'sid': Value(dtype='string', id=None)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 927/927 [00:00<00:00, 120993.30 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration: lt from the huggingface hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- lt\n",
      "{'entities': Sequence(feature={'end': Value(dtype='int32', id=None),\n",
      "                               'start': Value(dtype='int32', id=None),\n",
      "                               'text': Value(dtype='string', id=None),\n",
      "                               'type': ClassLabel(names=['Disorder and Finding',\n",
      "                                                         'Pharmaceutical Drug',\n",
      "                                                         'Body Structure'],\n",
      "                                                  id=None)},\n",
      "                      length=-1,\n",
      "                      id=None),\n",
      " 'sentence': Value(dtype='string', id=None),\n",
      " 'sid': Value(dtype='string', id=None)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 745753/745753 [00:00<00:00, 1103035.72 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration: wiki from the huggingface hub\n",
      "- wiki\n",
      "{'entities': Sequence(feature={'end': Value(dtype='int32', id=None),\n",
      "                               'start': Value(dtype='int32', id=None),\n",
      "                               'text': Value(dtype='string', id=None),\n",
      "                               'type': ClassLabel(names=['Disorder and Finding',\n",
      "                                                         'Pharmaceutical Drug',\n",
      "                                                         'Body Structure'],\n",
      "                                                  id=None)},\n",
      "                      length=-1,\n",
      "                      id=None),\n",
      " 'sentence': Value(dtype='string', id=None),\n",
      " 'sid': Value(dtype='string', id=None)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 48720/48720 [00:00<00:00, 790270.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# The dataset is loaded with all the chosen configurations\n",
    "\n",
    "kb_datasets =[]\n",
    "for config in configs:\n",
    "    if os.path.isfile(f\"data/swedish_medical_ner_{config}\"):\n",
    "        print(f\"Loading configuration: {config} from disk\")\n",
    "        ds = load_from_disk(f\"data/swedish_medical_ner_{config}\")\n",
    "        print(f\"- {ds.config_name}\") # Redundant?\n",
    "        pprint(ds[\"train\"].features) #Display schema    \n",
    "        kb_datasets.append(ds)\n",
    "    else:\n",
    "        print(f\"Loading configuration: {config} from the huggingface hub\")\n",
    "        ds = load_dataset(\"community-datasets/swedish_medical_ner\", config)\n",
    "        ds.config_name = config  # Attach config name as an attribute for later access. Not sure if we need this\n",
    "        print(f\"- {ds.config_name}\")\n",
    "        pprint(ds[\"train\"].features) #Display schema\n",
    "        kb_datasets.append(ds)\n",
    "        #Saving to disk\n",
    "        ds.save_to_disk(f\"data/swedish_medical_ner_{ds.config_name}\")\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df42e6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 datasets loaded.\n"
     ]
    }
   ],
   "source": [
    "# A check\n",
    "print(len(kb_datasets), \"datasets loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81909adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Configuration 1: 1177\n",
      "Rows: 927\n",
      "Columns: 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sid</th>\n",
       "      <th>sentence</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1177_0</td>\n",
       "      <td>Memantin ( Ebixa ) ger sällan några biverkningar.</td>\n",
       "      <td>{'start': [9], 'end': [18], 'text': ['Ebixa'], 'type': [0]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1177_1</td>\n",
       "      <td>Det är också lättare att dosera [ flytande medicin ] än att dela på tabletter.</td>\n",
       "      <td>{'start': [32], 'end': [52], 'text': ['flytande medicin'], 'type': [1]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1177_2</td>\n",
       "      <td>( Förstoppning ) är ett vanligt problem hos äldre.</td>\n",
       "      <td>{'start': [0], 'end': [16], 'text': ['Förstoppning'], 'type': [0]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1177_3</td>\n",
       "      <td>[ Medicinen ] kan också göra att man blöder lättare eftersom den påverkar { blodets } förmåga att levra sig.</td>\n",
       "      <td>{'start': [0, 74], 'end': [13, 85], 'text': ['Medicinen', 'blodets'], 'type': [1, 2]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1177_4</td>\n",
       "      <td>Barn har större möjligheter att samarbeta om de i förväg får veta vad som ska hända.</td>\n",
       "      <td>{'start': [], 'end': [], 'text': [], 'type': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1177_5</td>\n",
       "      <td>Eftersom de påverkar hela kroppen mer än övriga mediciner bör man bara ta dem när olika kombinationer av receptfria mediciner inte hjälper.</td>\n",
       "      <td>{'start': [], 'end': [], 'text': [], 'type': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1177_6</td>\n",
       "      <td>För att få ett skydd mot ( hepatit B ) behövs tre doser vaccin.</td>\n",
       "      <td>{'start': [25], 'end': [38], 'text': ['hepatit B'], 'type': [0]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1177_7</td>\n",
       "      <td>Effekten av naproxen sitter i längre och varar cirka 12 timmar jämfört med cirka 6 timmar för ibuprofen.</td>\n",
       "      <td>{'start': [], 'end': [], 'text': [], 'type': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1177_8</td>\n",
       "      <td>[ Cox-hämmare ] finns även som gel och sprej.</td>\n",
       "      <td>{'start': [0], 'end': [15], 'text': ['Cox-hämmare'], 'type': [1]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1177_9</td>\n",
       "      <td>Det är bra om ett litet barn är mätt och utsövt, eftersom de flesta påfrestningar då känns mindre.</td>\n",
       "      <td>{'start': [], 'end': [], 'text': [], 'type': []}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sid  \\\n",
       "0  1177_0   \n",
       "1  1177_1   \n",
       "2  1177_2   \n",
       "3  1177_3   \n",
       "4  1177_4   \n",
       "5  1177_5   \n",
       "6  1177_6   \n",
       "7  1177_7   \n",
       "8  1177_8   \n",
       "9  1177_9   \n",
       "\n",
       "                                                                                                                                      sentence  \\\n",
       "0                                                                                            Memantin ( Ebixa ) ger sällan några biverkningar.   \n",
       "1                                                               Det är också lättare att dosera [ flytande medicin ] än att dela på tabletter.   \n",
       "2                                                                                           ( Förstoppning ) är ett vanligt problem hos äldre.   \n",
       "3                                 [ Medicinen ] kan också göra att man blöder lättare eftersom den påverkar { blodets } förmåga att levra sig.   \n",
       "4                                                         Barn har större möjligheter att samarbeta om de i förväg får veta vad som ska hända.   \n",
       "5  Eftersom de påverkar hela kroppen mer än övriga mediciner bör man bara ta dem när olika kombinationer av receptfria mediciner inte hjälper.   \n",
       "6                                                                              För att få ett skydd mot ( hepatit B ) behövs tre doser vaccin.   \n",
       "7                                     Effekten av naproxen sitter i längre och varar cirka 12 timmar jämfört med cirka 6 timmar för ibuprofen.   \n",
       "8                                                                                                [ Cox-hämmare ] finns även som gel och sprej.   \n",
       "9                                           Det är bra om ett litet barn är mätt och utsövt, eftersom de flesta påfrestningar då känns mindre.   \n",
       "\n",
       "                                                                                entities  \n",
       "0                            {'start': [9], 'end': [18], 'text': ['Ebixa'], 'type': [0]}  \n",
       "1                {'start': [32], 'end': [52], 'text': ['flytande medicin'], 'type': [1]}  \n",
       "2                     {'start': [0], 'end': [16], 'text': ['Förstoppning'], 'type': [0]}  \n",
       "3  {'start': [0, 74], 'end': [13, 85], 'text': ['Medicinen', 'blodets'], 'type': [1, 2]}  \n",
       "4                                       {'start': [], 'end': [], 'text': [], 'type': []}  \n",
       "5                                       {'start': [], 'end': [], 'text': [], 'type': []}  \n",
       "6                       {'start': [25], 'end': [38], 'text': ['hepatit B'], 'type': [0]}  \n",
       "7                                       {'start': [], 'end': [], 'text': [], 'type': []}  \n",
       "8                      {'start': [0], 'end': [15], 'text': ['Cox-hämmare'], 'type': [1]}  \n",
       "9                                       {'start': [], 'end': [], 'text': [], 'type': []}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Configuration 2: lt\n",
      "Rows: 745753\n",
      "Columns: 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sid</th>\n",
       "      <th>sentence</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lt_0</td>\n",
       "      <td>, (hjärtinfarkt) och (syndrom) som vi nu år 1999 inte ens vet na</td>\n",
       "      <td>{'start': [2, 21], 'end': [16, 30], 'text': ['hjärtinfarkt', 'syndrom'], 'type': [0, 0]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lt_1</td>\n",
       "      <td>tinernas goda effekt på morbiditeten är välkänd, och data hi</td>\n",
       "      <td>{'start': [], 'end': [], 'text': [], 'type': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lt_2</td>\n",
       "      <td>[sukralfat], [lakrits] och vismut) som kunde utgöra ett skydd öv</td>\n",
       "      <td>{'start': [0, 13], 'end': [11, 22], 'text': ['sukralfat', 'lakrits'], 'type': [1, 1]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lt_3</td>\n",
       "      <td>och tveksamhet {vad} gäller operationsindikationen kan man ha</td>\n",
       "      <td>{'start': [16], 'end': [21], 'text': ['vad'], 'type': [2]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lt_4</td>\n",
       "      <td>1989 blev en anmälningspliktig (sjukdom) enligt Smittskyddsla</td>\n",
       "      <td>{'start': [32], 'end': [41], 'text': ['sjukdom'], 'type': [0]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lt_5</td>\n",
       "      <td>kombinerat med remodellering av (hjärtat). Detta säkras genom</td>\n",
       "      <td>{'start': [32], 'end': [41], 'text': ['hjärtat'], 'type': [0]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lt_6</td>\n",
       "      <td>olyckshändelse radikalt förändrat deras liv. {Sigmoideum} är</td>\n",
       "      <td>{'start': [46], 'end': [58], 'text': ['Sigmoideum'], 'type': [2]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lt_7</td>\n",
       "      <td>ra att hon samtidigt ordinerade [Cyklokapron] i en mängd av 5</td>\n",
       "      <td>{'start': [32], 'end': [45], 'text': ['Cyklokapron'], 'type': [1]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lt_8</td>\n",
       "      <td>till vara erfarenheterna och föra ut kunskapen till sjukvård</td>\n",
       "      <td>{'start': [], 'end': [], 'text': [], 'type': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lt_9</td>\n",
       "      <td>es kring behandling med betablockad vid (kronisk hjärtsvikt).</td>\n",
       "      <td>{'start': [40], 'end': [60], 'text': ['kronisk hjärtsvikt'], 'type': [0]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sid                                                          sentence  \\\n",
       "0  lt_0  , (hjärtinfarkt) och (syndrom) som vi nu år 1999 inte ens vet na   \n",
       "1  lt_1      tinernas goda effekt på morbiditeten är välkänd, och data hi   \n",
       "2  lt_2  [sukralfat], [lakrits] och vismut) som kunde utgöra ett skydd öv   \n",
       "3  lt_3     och tveksamhet {vad} gäller operationsindikationen kan man ha   \n",
       "4  lt_4     1989 blev en anmälningspliktig (sjukdom) enligt Smittskyddsla   \n",
       "5  lt_5    kombinerat med remodellering av (hjärtat). Detta säkras genom    \n",
       "6  lt_6     olyckshändelse radikalt förändrat deras liv. {Sigmoideum} är    \n",
       "7  lt_7    ra att hon samtidigt ordinerade [Cyklokapron] i en mängd av 5    \n",
       "8  lt_8      till vara erfarenheterna och föra ut kunskapen till sjukvård   \n",
       "9  lt_9    es kring behandling med betablockad vid (kronisk hjärtsvikt).    \n",
       "\n",
       "                                                                                   entities  \n",
       "0  {'start': [2, 21], 'end': [16, 30], 'text': ['hjärtinfarkt', 'syndrom'], 'type': [0, 0]}  \n",
       "1                                          {'start': [], 'end': [], 'text': [], 'type': []}  \n",
       "2     {'start': [0, 13], 'end': [11, 22], 'text': ['sukralfat', 'lakrits'], 'type': [1, 1]}  \n",
       "3                                {'start': [16], 'end': [21], 'text': ['vad'], 'type': [2]}  \n",
       "4                            {'start': [32], 'end': [41], 'text': ['sjukdom'], 'type': [0]}  \n",
       "5                            {'start': [32], 'end': [41], 'text': ['hjärtat'], 'type': [0]}  \n",
       "6                         {'start': [46], 'end': [58], 'text': ['Sigmoideum'], 'type': [2]}  \n",
       "7                        {'start': [32], 'end': [45], 'text': ['Cyklokapron'], 'type': [1]}  \n",
       "8                                          {'start': [], 'end': [], 'text': [], 'type': []}  \n",
       "9                 {'start': [40], 'end': [60], 'text': ['kronisk hjärtsvikt'], 'type': [0]}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Configuration 3: wiki\n",
      "Rows: 48720\n",
      "Columns: 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sid</th>\n",
       "      <th>sentence</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wiki_0</td>\n",
       "      <td>{kropp} beskrivs i till exempel människokroppen, anatomi och f</td>\n",
       "      <td>{'start': [0], 'end': [7], 'text': ['kropp'], 'type': [2]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wiki_1</td>\n",
       "      <td>sju miljoner år gammalt hominint {kranium}, klassificerad som</td>\n",
       "      <td>{'start': [33], 'end': [42], 'text': ['kranium'], 'type': [2]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wiki_2</td>\n",
       "      <td>autosomer och ett par könskromosomer. Varje {kromosom} består</td>\n",
       "      <td>{'start': [45], 'end': [55], 'text': ['kromosom'], 'type': [2]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wiki_3</td>\n",
       "      <td>{kromosom} består av en DNA-molekyl och {protein}. En DNA-molek</td>\n",
       "      <td>{'start': [1], 'end': [50], 'text': ['kromosom} består av en DNA-molekyl och {protein'], 'type': [2]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wiki_4</td>\n",
       "      <td>tikel:Människans {skelett} Människans skelett är det skelett s</td>\n",
       "      <td>{'start': [17], 'end': [26], 'text': ['skelett'], 'type': [2]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wiki_5</td>\n",
       "      <td>os människor. En vuxen människas {skelett} består av 206 till</td>\n",
       "      <td>{'start': [33], 'end': [42], 'text': ['skelett'], 'type': [2]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wiki_6</td>\n",
       "      <td>{lett} består av 206 till 220 {ben}, beroende på hur man räknar.</td>\n",
       "      <td>{'start': [0], 'end': [35], 'text': ['lett} består av 206 till 220 {ben'], 'type': [2]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wiki_7</td>\n",
       "      <td>v kroppsvikten.Ett nyfött barn har ca 300 {ben} i kroppen vilk</td>\n",
       "      <td>{'start': [42], 'end': [47], 'text': ['ben'], 'type': [2]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wiki_8</td>\n",
       "      <td>kollektivet i mindre bitar såsom länder &gt; städer &gt; orter {Hud}</td>\n",
       "      <td>{'start': [57], 'end': [62], 'text': ['Hud'], 'type': [2]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>wiki_9</td>\n",
       "      <td>sdjur. {Huden} utgör ett mekaniskt skydd mot omvärlden och bid</td>\n",
       "      <td>{'start': [7], 'end': [14], 'text': ['Huden'], 'type': [2]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sid                                                          sentence  \\\n",
       "0  wiki_0    {kropp} beskrivs i till exempel människokroppen, anatomi och f   \n",
       "1  wiki_1    sju miljoner år gammalt hominint {kranium}, klassificerad som    \n",
       "2  wiki_2     autosomer och ett par könskromosomer. Varje {kromosom} består   \n",
       "3  wiki_3   {kromosom} består av en DNA-molekyl och {protein}. En DNA-molek   \n",
       "4  wiki_4    tikel:Människans {skelett} Människans skelett är det skelett s   \n",
       "5  wiki_5    os människor. En vuxen människas {skelett} består av 206 till    \n",
       "6  wiki_6  {lett} består av 206 till 220 {ben}, beroende på hur man räknar.   \n",
       "7  wiki_7    v kroppsvikten.Ett nyfött barn har ca 300 {ben} i kroppen vilk   \n",
       "8  wiki_8    kollektivet i mindre bitar såsom länder > städer > orter {Hud}   \n",
       "9  wiki_9    sdjur. {Huden} utgör ett mekaniskt skydd mot omvärlden och bid   \n",
       "\n",
       "                                                                                                entities  \n",
       "0                                             {'start': [0], 'end': [7], 'text': ['kropp'], 'type': [2]}  \n",
       "1                                         {'start': [33], 'end': [42], 'text': ['kranium'], 'type': [2]}  \n",
       "2                                        {'start': [45], 'end': [55], 'text': ['kromosom'], 'type': [2]}  \n",
       "3  {'start': [1], 'end': [50], 'text': ['kromosom} består av en DNA-molekyl och {protein'], 'type': [2]}  \n",
       "4                                         {'start': [17], 'end': [26], 'text': ['skelett'], 'type': [2]}  \n",
       "5                                         {'start': [33], 'end': [42], 'text': ['skelett'], 'type': [2]}  \n",
       "6                {'start': [0], 'end': [35], 'text': ['lett} består av 206 till 220 {ben'], 'type': [2]}  \n",
       "7                                             {'start': [42], 'end': [47], 'text': ['ben'], 'type': [2]}  \n",
       "8                                             {'start': [57], 'end': [62], 'text': ['Hud'], 'type': [2]}  \n",
       "9                                            {'start': [7], 'end': [14], 'text': ['Huden'], 'type': [2]}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The first configuration is explored\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "for i, dataset in enumerate(kb_datasets):\n",
    "    print(f\"### Configuration {i + 1}: {dataset.config_name}\")\n",
    "    print(f\"Rows: {dataset['train'].num_rows}\")  # Shows splits and number of examples\n",
    "    print(f\"Columns: {len(dataset['train'].features)}\")  # Number of columns/features\n",
    "\n",
    "    # Convert a slice of the dataset to a pandas dataframe and display it\n",
    "    example = dataset[\"train\"].select(range(10)).to_pandas()\n",
    "    display(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d6b6cb",
   "metadata": {},
   "source": [
    "The data of interest are: the Passage text, the Named entities and their types.\n",
    "Named entities consists of spans, a contiguous sequence of tokens (words, subwords, or characters) in a text that together represent a single entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "effa739a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1177': DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sid', 'sentence', 'entities', 'source'],\n",
      "        num_rows: 880\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sid', 'sentence', 'entities', 'source'],\n",
      "        num_rows: 47\n",
      "    })\n",
      "}), 'lt': DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sid', 'sentence', 'entities', 'source'],\n",
      "        num_rows: 708465\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sid', 'sentence', 'entities', 'source'],\n",
      "        num_rows: 37288\n",
      "    })\n",
      "}), 'wiki': DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sid', 'sentence', 'entities', 'source'],\n",
      "        num_rows: 46284\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sid', 'sentence', 'entities', 'source'],\n",
      "        num_rows: 2436\n",
      "    })\n",
      "})}\n"
     ]
    }
   ],
   "source": [
    "# 1) Split each config into train/val\n",
    "per_source_raw= split_sources(kb_datasets, val_fraction=0.05, seed=42)\n",
    "print(per_source_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2ff2018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-body_structure', 'I-body_structure', 'B-disorder_finding', 'I-disorder_finding', 'B-pharmaceutical_drug', 'I-pharmaceutical_drug']\n",
      "{'O': 0, 'B-body_structure': 1, 'I-body_structure': 2, 'B-disorder_finding': 3, 'I-disorder_finding': 4, 'B-pharmaceutical_drug': 5, 'I-pharmaceutical_drug': 6}\n",
      "{0: 'O', 1: 'B-body_structure', 2: 'I-body_structure', 3: 'B-disorder_finding', 4: 'I-disorder_finding', 5: 'B-pharmaceutical_drug', 6: 'I-pharmaceutical_drug'}\n"
     ]
    }
   ],
   "source": [
    "# 2) Build global BIO labels (union over configs)\n",
    "# label_list = build_bio_label_list_from_sources(per_source_raw)\n",
    "# label2id = {l: i for i, l in enumerate(label_list)}\n",
    "# id2label = {i: l for l, i in label2id.items()}\n",
    "label_list = build_bio_label_list_from_sources(per_source_raw)\n",
    "label2id = {l: i for i, l in enumerate(label_list)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "print(label_list)\n",
    "print(label2id)\n",
    "print(id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51e5528b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at KB/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 3) Model & tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KB/bert-base-swedish-cased\", use_fast=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"KB/bert-base-swedish-cased\",\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "compute_metrics = compute_metrics_factory(id2label=id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b676eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 880/880 [00:00<00:00, 13352.73 examples/s]\n",
      "Map: 100%|██████████| 47/47 [00:00<00:00, 5743.11 examples/s]\n",
      "Map: 100%|██████████| 880/880 [00:00<00:00, 10560.58 examples/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid string class label pharmaceutical_drug",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3547\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3546\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m update_data \u001b[38;5;129;01mand\u001b[39;00m writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3547\u001b[39m         \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# close_stream=bool(buf_writer is None))  # We only close if we are writing in a file\u001b[39;00m\n\u001b[32m   3548\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/datasets/arrow_writer.py:657\u001b[39m, in \u001b[36mArrowWriter.finalize\u001b[39m\u001b[34m(self, close_stream)\u001b[39m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28mself\u001b[39m.hkey_record = []\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_examples_on_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;66;03m# If schema is known, infer features even if no examples were written\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/datasets/arrow_writer.py:510\u001b[39m, in \u001b[36mArrowWriter.write_examples_on_file\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    506\u001b[39m         batch_examples[col] = [\n\u001b[32m    507\u001b[39m             row[\u001b[32m0\u001b[39m][col].to_pylist()[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row[\u001b[32m0\u001b[39m][col], (pa.Array, pa.ChunkedArray)) \u001b[38;5;28;01melse\u001b[39;00m row[\u001b[32m0\u001b[39m][col]\n\u001b[32m    508\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_examples\n\u001b[32m    509\u001b[39m         ]\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_examples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_examples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[38;5;28mself\u001b[39m.current_examples = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/datasets/arrow_writer.py:626\u001b[39m, in \u001b[36mArrowWriter.write_batch\u001b[39m\u001b[34m(self, batch_examples, writer_batch_size, try_original_type)\u001b[39m\n\u001b[32m    625\u001b[39m typed_sequence = OptimizedTypedSequence(col_values, \u001b[38;5;28mtype\u001b[39m=col_type, try_type=col_try_type, col=col)\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m arrays.append(\u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyped_sequence\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    627\u001b[39m inferred_features[col] = typed_sequence.get_inferred_type()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/pyarrow/array.pxi:256\u001b[39m, in \u001b[36mpyarrow.lib.array\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/pyarrow/array.pxi:118\u001b[39m, in \u001b[36mpyarrow.lib._handle_arrow_array_protocol\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/datasets/arrow_writer.py:258\u001b[39m, in \u001b[36mTypedSequence.__arrow_array__\u001b[39m\u001b[34m(self, type)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    255\u001b[39m     \u001b[38;5;66;03m# We use cast_array_to_feature to support casting to custom types like Audio and Image\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;66;03m# Also, when trying type \"string\", we don't want to convert integers or floats to \"string\".\u001b[39;00m\n\u001b[32m    257\u001b[39m     \u001b[38;5;66;03m# We only do it if trying_type is False - since this is what the user asks for.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     out = \u001b[43mcast_array_to_feature\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_primitive_to_str\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrying_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_decimal_to_str\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrying_type\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/datasets/table.py:1798\u001b[39m, in \u001b[36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[39m\u001b[34m(array, *args, **kwargs)\u001b[39m\n\u001b[32m   1797\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1798\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/datasets/table.py:2066\u001b[39m, in \u001b[36mcast_array_to_feature\u001b[39m\u001b[34m(array, feature, allow_primitive_to_str, allow_decimal_to_str)\u001b[39m\n\u001b[32m   2065\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2066\u001b[39m     casted_array_values = \u001b[43m_c\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2067\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pa.types.is_list(array.type) \u001b[38;5;129;01mand\u001b[39;00m casted_array_values.type == array.values.type:\n\u001b[32m   2068\u001b[39m         \u001b[38;5;66;03m# Both array and feature have equal list type and values (within the list) type\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/datasets/table.py:1798\u001b[39m, in \u001b[36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[39m\u001b[34m(array, *args, **kwargs)\u001b[39m\n\u001b[32m   1797\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1798\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/datasets/table.py:2007\u001b[39m, in \u001b[36mcast_array_to_feature\u001b[39m\u001b[34m(array, feature, allow_primitive_to_str, allow_decimal_to_str)\u001b[39m\n\u001b[32m   2005\u001b[39m null_array = pa.array([\u001b[38;5;28;01mNone\u001b[39;00m] * \u001b[38;5;28mlen\u001b[39m(array))\n\u001b[32m   2006\u001b[39m arrays = [\n\u001b[32m-> \u001b[39m\u001b[32m2007\u001b[39m     \u001b[43m_c\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marray_fields\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnull_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubfeature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2008\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, subfeature \u001b[38;5;129;01min\u001b[39;00m feature.items()\n\u001b[32m   2009\u001b[39m ]\n\u001b[32m   2010\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pa.StructArray.from_arrays(arrays, names=\u001b[38;5;28mlist\u001b[39m(feature), mask=array.is_null())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/datasets/table.py:1798\u001b[39m, in \u001b[36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[39m\u001b[34m(array, *args, **kwargs)\u001b[39m\n\u001b[32m   1797\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1798\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/datasets/table.py:1996\u001b[39m, in \u001b[36mcast_array_to_feature\u001b[39m\u001b[34m(array, feature, allow_primitive_to_str, allow_decimal_to_str)\u001b[39m\n\u001b[32m   1995\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(feature, \u001b[33m\"\u001b[39m\u001b[33mcast_storage\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1996\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfeature\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1998\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m pa.types.is_struct(array.type):\n\u001b[32m   1999\u001b[39m     \u001b[38;5;66;03m# feature must be a dict or Sequence(subfeatures_dict)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/datasets/features/features.py:1141\u001b[39m, in \u001b[36mClassLabel.cast_storage\u001b[39m\u001b[34m(self, storage)\u001b[39m\n\u001b[32m   1139\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(storage, pa.StringArray):\n\u001b[32m   1140\u001b[39m     storage = pa.array(\n\u001b[32m-> \u001b[39m\u001b[32m1141\u001b[39m         [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_strval2int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m storage.to_pylist()]\n\u001b[32m   1142\u001b[39m     )\n\u001b[32m   1143\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m array_cast(storage, \u001b[38;5;28mself\u001b[39m.pa_type)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/datasets/features/features.py:1070\u001b[39m, in \u001b[36mClassLabel._strval2int\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m   1069\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failed_parse:\n\u001b[32m-> \u001b[39m\u001b[32m1070\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid string class label \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m int_value\n",
      "\u001b[31mValueError\u001b[39m: Invalid string class label pharmaceutical_drug",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m per_source_norm[cfg] = ds.map(\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m x: x,  \u001b[38;5;66;03m# no-op to get a copy\u001b[39;00m\n\u001b[32m      5\u001b[39m )\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m per_source_norm[cfg].keys():\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     per_source_norm[cfg][split] = \u001b[43mnormalize_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mper_source_norm\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/src/ner/utility_functions.py:195\u001b[39m, in \u001b[36mnormalize_types\u001b[39m\u001b[34m(ds_split)\u001b[39m\n\u001b[32m    193\u001b[39m     ex[\u001b[33m\"\u001b[39m\u001b[33mentities\u001b[39m\u001b[33m\"\u001b[39m] = ents\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ex\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mds_split\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:557\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m self_format = {\n\u001b[32m    551\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    552\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    553\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    555\u001b[39m }\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3079\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3073\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3074\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[32m   3075\u001b[39m         unit=\u001b[33m\"\u001b[39m\u001b[33m examples\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3076\u001b[39m         total=pbar_total,\n\u001b[32m   3077\u001b[39m         desc=desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mMap\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3078\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m3079\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3552\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[32m   3551\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3552\u001b[39m         \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tmp_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3554\u001b[39m         tmp_file.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/datasets/arrow_writer.py:657\u001b[39m, in \u001b[36mArrowWriter.finalize\u001b[39m\u001b[34m(self, close_stream)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# Re-initializing to empty list for next batch\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28mself\u001b[39m.hkey_record = []\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_examples_on_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;66;03m# If schema is known, infer features even if no examples were written\u001b[39;00m\n\u001b[32m    659\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.schema:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/datasets/arrow_writer.py:510\u001b[39m, in \u001b[36mArrowWriter.write_examples_on_file\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    505\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    506\u001b[39m         batch_examples[col] = [\n\u001b[32m    507\u001b[39m             row[\u001b[32m0\u001b[39m][col].to_pylist()[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row[\u001b[32m0\u001b[39m][col], (pa.Array, pa.ChunkedArray)) \u001b[38;5;28;01melse\u001b[39;00m row[\u001b[32m0\u001b[39m][col]\n\u001b[32m    508\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_examples\n\u001b[32m    509\u001b[39m         ]\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_examples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_examples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[38;5;28mself\u001b[39m.current_examples = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/datasets/arrow_writer.py:626\u001b[39m, in \u001b[36mArrowWriter.write_batch\u001b[39m\u001b[34m(self, batch_examples, writer_batch_size, try_original_type)\u001b[39m\n\u001b[32m    620\u001b[39m         col_try_type = (\n\u001b[32m    621\u001b[39m             try_features[col]\n\u001b[32m    622\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m try_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m try_features \u001b[38;5;129;01mand\u001b[39;00m try_original_type\n\u001b[32m    623\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    624\u001b[39m         )\n\u001b[32m    625\u001b[39m         typed_sequence = OptimizedTypedSequence(col_values, \u001b[38;5;28mtype\u001b[39m=col_type, try_type=col_try_type, col=col)\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m         arrays.append(\u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyped_sequence\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    627\u001b[39m         inferred_features[col] = typed_sequence.get_inferred_type()\n\u001b[32m    628\u001b[39m schema = inferred_features.arrow_schema \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.schema\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/pyarrow/array.pxi:256\u001b[39m, in \u001b[36mpyarrow.lib.array\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/pyarrow/array.pxi:118\u001b[39m, in \u001b[36mpyarrow.lib._handle_arrow_array_protocol\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/datasets/arrow_writer.py:258\u001b[39m, in \u001b[36mTypedSequence.__arrow_array__\u001b[39m\u001b[34m(self, type)\u001b[39m\n\u001b[32m    253\u001b[39m     \u001b[38;5;66;03m# otherwise we can finally use the user's type\u001b[39;00m\n\u001b[32m    254\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    255\u001b[39m         \u001b[38;5;66;03m# We use cast_array_to_feature to support casting to custom types like Audio and Image\u001b[39;00m\n\u001b[32m    256\u001b[39m         \u001b[38;5;66;03m# Also, when trying type \"string\", we don't want to convert integers or floats to \"string\".\u001b[39;00m\n\u001b[32m    257\u001b[39m         \u001b[38;5;66;03m# We only do it if trying_type is False - since this is what the user asks for.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m         out = \u001b[43mcast_array_to_feature\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m            \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_primitive_to_str\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrying_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_decimal_to_str\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrying_type\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[32m    263\u001b[39m     \u001b[38;5;167;01mTypeError\u001b[39;00m,\n\u001b[32m    264\u001b[39m     pa.lib.ArrowInvalid,\n\u001b[32m    265\u001b[39m     pa.lib.ArrowNotImplementedError,\n\u001b[32m    266\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# handle type errors and overflows\u001b[39;00m\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m# Ignore ArrowNotImplementedError caused by trying type, otherwise re-raise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/datasets/table.py:1798\u001b[39m, in \u001b[36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[39m\u001b[34m(array, *args, **kwargs)\u001b[39m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pa.chunked_array([func(chunk, *args, **kwargs) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m array.chunks])\n\u001b[32m   1797\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1798\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/datasets/table.py:2066\u001b[39m, in \u001b[36mcast_array_to_feature\u001b[39m\u001b[34m(array, feature, allow_primitive_to_str, allow_decimal_to_str)\u001b[39m\n\u001b[32m   2064\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m pa.FixedSizeListArray.from_arrays(_c(array_values, feature.feature), feature.length)\n\u001b[32m   2065\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2066\u001b[39m     casted_array_values = \u001b[43m_c\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2067\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pa.types.is_list(array.type) \u001b[38;5;129;01mand\u001b[39;00m casted_array_values.type == array.values.type:\n\u001b[32m   2068\u001b[39m         \u001b[38;5;66;03m# Both array and feature have equal list type and values (within the list) type\u001b[39;00m\n\u001b[32m   2069\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m array\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/datasets/table.py:1798\u001b[39m, in \u001b[36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[39m\u001b[34m(array, *args, **kwargs)\u001b[39m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pa.chunked_array([func(chunk, *args, **kwargs) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m array.chunks])\n\u001b[32m   1797\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1798\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/datasets/table.py:2007\u001b[39m, in \u001b[36mcast_array_to_feature\u001b[39m\u001b[34m(array, feature, allow_primitive_to_str, allow_decimal_to_str)\u001b[39m\n\u001b[32m   2004\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feature, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (array_fields := {field.name \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m array.type}) <= \u001b[38;5;28mset\u001b[39m(feature):\n\u001b[32m   2005\u001b[39m         null_array = pa.array([\u001b[38;5;28;01mNone\u001b[39;00m] * \u001b[38;5;28mlen\u001b[39m(array))\n\u001b[32m   2006\u001b[39m         arrays = [\n\u001b[32m-> \u001b[39m\u001b[32m2007\u001b[39m             \u001b[43m_c\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marray_fields\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnull_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubfeature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2008\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m name, subfeature \u001b[38;5;129;01min\u001b[39;00m feature.items()\n\u001b[32m   2009\u001b[39m         ]\n\u001b[32m   2010\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m pa.StructArray.from_arrays(arrays, names=\u001b[38;5;28mlist\u001b[39m(feature), mask=array.is_null())\n\u001b[32m   2011\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m pa.types.is_list(array.type) \u001b[38;5;129;01mor\u001b[39;00m pa.types.is_large_list(array.type):\n\u001b[32m   2012\u001b[39m     \u001b[38;5;66;03m# feature must be either [subfeature] or LargeList(subfeature) or Sequence(subfeature)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/datasets/table.py:1798\u001b[39m, in \u001b[36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[39m\u001b[34m(array, *args, **kwargs)\u001b[39m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pa.chunked_array([func(chunk, *args, **kwargs) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m array.chunks])\n\u001b[32m   1797\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1798\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/datasets/table.py:1996\u001b[39m, in \u001b[36mcast_array_to_feature\u001b[39m\u001b[34m(array, feature, allow_primitive_to_str, allow_decimal_to_str)\u001b[39m\n\u001b[32m   1994\u001b[39m     array = array.storage\n\u001b[32m   1995\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(feature, \u001b[33m\"\u001b[39m\u001b[33mcast_storage\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1996\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfeature\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1998\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m pa.types.is_struct(array.type):\n\u001b[32m   1999\u001b[39m     \u001b[38;5;66;03m# feature must be a dict or Sequence(subfeatures_dict)\u001b[39;00m\n\u001b[32m   2000\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feature, Sequence) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feature.feature, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/datasets/features/features.py:1141\u001b[39m, in \u001b[36mClassLabel.cast_storage\u001b[39m\u001b[34m(self, storage)\u001b[39m\n\u001b[32m   1136\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1137\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mClass label \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmin_max[\u001b[33m'\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m greater than configured num_classes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1138\u001b[39m         )\n\u001b[32m   1139\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(storage, pa.StringArray):\n\u001b[32m   1140\u001b[39m     storage = pa.array(\n\u001b[32m-> \u001b[39m\u001b[32m1141\u001b[39m         [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_strval2int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m storage.to_pylist()]\n\u001b[32m   1142\u001b[39m     )\n\u001b[32m   1143\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m array_cast(storage, \u001b[38;5;28mself\u001b[39m.pa_type)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CM2011/CM2011-NER-Entity-Linking/.venv/lib/python3.12/site-packages/datasets/features/features.py:1070\u001b[39m, in \u001b[36mClassLabel._strval2int\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m   1068\u001b[39m                 failed_parse = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1069\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failed_parse:\n\u001b[32m-> \u001b[39m\u001b[32m1070\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid string class label \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m int_value\n",
      "\u001b[31mValueError\u001b[39m: Invalid string class label pharmaceutical_drug"
     ]
    }
   ],
   "source": [
    "per_source_norm = {}\n",
    "for cfg, ds in per_source_raw.items():\n",
    "    per_source_norm[cfg] = ds.map(\n",
    "        lambda x: x,  # no-op to get a copy\n",
    "    )\n",
    "    for split in per_source_norm[cfg].keys():\n",
    "        per_source_norm[cfg][split] = normalize_types(per_source_norm[cfg][split])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e5d53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Convert span schema → token-classification features\n",
    "to_features = make_to_features(tokenizer, label2id, max_length=256)\n",
    "per_source = process_all(per_source_raw, to_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33465a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Prepare TrainingArguments base\n",
    "base_args = dict(\n",
    "    output_dir=\"outputs/ner_kbbert_multi\",\n",
    "    learning_rate=2e-5, # Only used for interleave mode.\"\n",
    "    num_train_epochs=2.0, # Only used for interleave mode.\"\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=False,\n",
    "    seed=42,\n",
    "    report_to=[],  # disable HF trackers by default\n",
    "    fp16=False, # fp16=True,\n",
    "    bf16=False, # bf16=True,\n",
    "    gradient_checkpointing=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea4694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Staged curriculum params (epochs & LR per stage)\n",
    "stage_lt_epochs = 1.0\n",
    "stage_lt_lr = 2e-5\n",
    "stage_wiki_epochs=1.0\n",
    "stage_wiki_lr=1e-5\n",
    "stage_1177_epochs=2.0\n",
    "stage_1177_lr = 5e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ec862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Train\n",
    "# Staged curriculum: lt -> wiki -> 1177 (default order if present)\n",
    "order = []\n",
    "# map short names to full config ids if present\n",
    "short2cfg = { \"lt\":None, \"wiki\":None, \"1177\":None }\n",
    "cfg= None\n",
    "for kb_dataset in kb_datasets:\n",
    "    if \"lt\" in kb_dataset.config_name: short2cfg[\"lt\"] = cfg\n",
    "    elif \"wiki\" in kb_dataset.config_name: short2cfg[\"wiki\"] = cfg\n",
    "    elif \"1177\" in kb_dataset.config_name: short2cfg[\"1177\"] = cfg\n",
    "if short2cfg[\"lt\"]:   order.append((\"lt\", short2cfg[\"lt\"], stage_lt_epochs, stage_lt_lr))\n",
    "if short2cfg[\"wiki\"]: order.append((\"wiki\", short2cfg[\"wiki\"], stage_wiki_epochs, stage_wiki_lr))\n",
    "if short2cfg[\"1177\"]: order.append((\"1177\", short2cfg[\"1177\"], stage_1177_epochs, stage_1177_lr))\n",
    "if not order:\n",
    "    raise ValueError(\"Could not infer stages from dataset_configs. Include lt/wiki/1177 source configs.\")\n",
    "\n",
    "for stage_name, cfg, epochs, lr in order:\n",
    "    #logger.info(f\"\\n=== Stage: {stage_name} on {cfg} | epochs={epochs} lr={lr} ===\")\n",
    "    stage_args = TrainingArguments(\n",
    "        **{**base_args,\n",
    "            \"num_train_epochs\": epochs,\n",
    "            \"learning_rate\": lr,\n",
    "            \"output_dir\": os.path.join(base_args[\"output_dir\"], f\"stage_{stage_name}\")},\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=stage_args,\n",
    "        train_dataset=per_source[cfg][\"train\"],\n",
    "        eval_dataset=per_source[cfg][\"validation\"],\n",
    "        tokenizer=tokenizer,             # (works on 4.x; future deprec warns ok)\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    model = trainer.model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
